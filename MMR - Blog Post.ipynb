{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Math \"Rock\"\n",
    "\n",
    "Generative models are all the rage right now, and I've always been fascinated with them.\n",
    "\n",
    "I've been playing and listening to music for most of my life, and the more music theory I learn, the more it becomes apparent how much structure there is in music.\n",
    "\n",
    "For this project I'll attempt to generate songs in the math rock genre using, well, math. [Math rock](https://en.wikipedia.org/wiki/Math_rock) is one of my favorite genres of music - it's a modern progressive style of rock that often features elements such as [odd time signatures](https://en.wikipedia.org/wiki/Time_signature#Complex_time_signatures), [polyrhythm](https://en.wikipedia.org/wiki/Polyrhythm), [counterpoint](https://en.wikipedia.org/wiki/Counterpoint), unconventional song structures, and so on. [Melody 4 by Tera Melos](https://open.spotify.com/track/0JeVTUELKzEIsPtEWd1VDU?si=5f9d43510d4f4dce) is, in my opinion, a great example that characterizes many elements of the genre. It's not for everyone, and some say it just sounds like noise. This is beneficial because the results of the model are likely to be just noise, but the underlying structure of the songs used in the training data is going to be much more difficult to learn than a conventional rock song.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Something about what all is happening with this and why\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Convert mp3 files to MIDI files with Spotify's [Basic Pitch](https://github.com/spotify/basic-pitch) library\n",
    "2. Convert MIDI files to a NumPy array\n",
    "3. Train a model to predict the note(s) at the next time step\n",
    "4. Generate songs by passing a seed song snippet to the model and then continuously predict the note(s) at the next time step for a pre-determined amount of time\n",
    "5. Convert the generated song from a NumPy array to MIDI\n",
    "6. Convert the MIDI file to an audio file\n",
    "7. Add special effects with Spotify's [Pedalboard](https://github.com/spotify/pedalboard) library\n",
    "\n",
    "\n",
    "### Shortcomings\n",
    "\n",
    "Unfortunately my training data for this project isn't great. This requires mp3 files, and I mostly started listening to math rock after I got a Spotify subscription and stopped purchasing mp3s.\n",
    "\n",
    "The processing power for training the model is also limited. I'm training this on a desktop without a GPU, so \n",
    "\n",
    "### Other ideas\n",
    "\n",
    "- Genre blending\n",
    "- Compare to a song generated from chatGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Separate main script out into other scripts\n",
    "#   - Training the model\n",
    "# - Fix audio to MIDI conversion for ones not working\n",
    "# - Re-train the model with azure\n",
    "#   - Train model\n",
    "#       - Try w/ VAEs\n",
    "#   - Save & Download model\n",
    "# - Figure out num notes to use for training sequence\n",
    "# - See if we can augment data with other things - key, tempo, time signature, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting mp3 files to MIDI\n",
    "\n",
    "We'll do this with Spotify's [Basic Pitch](https://github.com/spotify/basic-pitch) library. This is what originally sparked the idea for this project because it made it feasible and easy to turn audio files into a numerical representation.\n",
    "\n",
    "This library uses a model to predict the the notes being played - specifically the start/stop times, pitch, and the velocity (how hard a note is played). It's surprisingly fast and lightweight, and it only took a few minutes to convert the songs I wanted to use for my training data.\n",
    "\n",
    "If you want to learn more, I recommend listening to the [Spotify Engineering Podcast (NerdOut)](https://open.spotify.com/show/5eXZwvvxt3K2dxha3BSaAe) episode on [Basic Pitch](https://open.spotify.com/episode/4wDDgWn037xjuq4Hr0u6a3?si=8a93b14952d546ca) and checking out the [announcement post](https://engineering.atspotify.com/2022/06/meet-basic-pitch/) and [website](https://basicpitch.spotify.com/) for it.\n",
    "\n",
    "We'll start with some library imports and then grab the paths to the MP3 files we want to use for the training set before using basic pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from basic_pitch import inference as basic_pitch_inference\n",
    "from basic_pitch import ICASSP_2022_MODEL_PATH  # Recommended when predicting for multiple songs\n",
    "\n",
    "# Setting up the directories\n",
    "cwd = os.getcwd()\n",
    "mp3_directory = os.path.join(cwd, \"Data/Songs/\")\n",
    "midi_directory = os.path.join(cwd, \"Data/MIDIs/\")\n",
    "\n",
    "# Renaming all mp3s to numbers\n",
    "mp3_index = 0\n",
    "for root, dirs, files in os.walk(mp3_directory):\n",
    "    for file in files:\n",
    "        filepath = root + \"\\\\\" + file\n",
    "        if filepath.lower().endswith(\".mp3\"):\n",
    "            new_filepath = root + \"\\\\\" + str(mp3_index) + \".mp3\"\n",
    "            mp3_index += 1\n",
    "            os.rename(filepath, new_filepath)\n",
    "\n",
    "# Getting a list of already converted MIDI files and the remaining mp3s that need to be converted\n",
    "mp3s = []\n",
    "midis = []\n",
    "for root, dirs, files in os.walk(midi_directory):\n",
    "    for file in files:\n",
    "        filepath = root + \"\\\\\" + file\n",
    "        if filepath.lower().endswith(\".mid\"):\n",
    "            midis.append(filepath)\n",
    "for root, dirs, files in os.walk(mp3_directory):\n",
    "    for file in files:\n",
    "        filepath = root + \"\\\\\" + file\n",
    "        if filepath.lower().endswith(\".mp3\"):\n",
    "            mp3s.append(filepath)\n",
    "\n",
    "# Removing mp3s that have already been converted\n",
    "mp3s = [mp3 for mp3 in mp3s if mp3.replace(\".mp3\", \".mid\").replace(\"/Songs/\", \"/MIDIs/\") not in midis]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use the basic pitch model to convert the mp3 files to MIDI files and save them in the MIDI directory. We'll also log any errors that occur into a separate file in case we run into issues and need to debug them.\n",
    "\n",
    "It's important to note that the basic pitch model also outputs the notes, but we will not be using that here because we can extract them from the MIDI files and get additional information (e.g. the key and time signature) using other libraries like [music21](https://github.com/cuthbertLab/music21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the mp3s, converting them to MIDI, and saving them to the MIDI directory\n",
    "basic_pitch_model = tf.saved_model.load(str(ICASSP_2022_MODEL_PATH))\n",
    "log_file = open(\"Data/Outputs/processFailures.log\", \"w\")  # To report issues\n",
    "num_processed_songs = 0  # For reporting overall results\n",
    "for i, mp3 in enumerate(mp3s):\n",
    "    print(f\"{i / len(mp3s):.0%}\")\n",
    "    mp3_to_convert = mp3s[i]\n",
    "    mp3 = mp3.replace(\"\\\\\", \"/\")\n",
    "\n",
    "    # Using basic pitch model to convert mp3 to MIDI\n",
    "    try:\n",
    "        _, midi_data, _ = basic_pitch_inference.predict(mp3, basic_pitch_model)\n",
    "        midi_path = mp3.replace(\"/Songs/\", \"/MIDIs/\").replace(\".mp3\", \".mid\")\n",
    "        midi_data.write(midi_path)  # Saving the MIDI file\n",
    "        midis.append(midi_path)  # Adding the MIDI file to the list of MIDI files\n",
    "        num_processed_songs += 1\n",
    "\n",
    "    # Logging errors that occur\n",
    "    except Exception as e:\n",
    "        print(f\"Issue with {mp3}\")\n",
    "        log_file.write(f\"{mp3}: {str(e)}\\n\")\n",
    "        pass\n",
    "log_file.close()\n",
    "\n",
    "# Reporting the results\n",
    "num_processed_songs = num_processed_songs\n",
    "num_total_songs = len(mp3s)\n",
    "print(\n",
    "    f\"Successfully processed {num_processed_songs}  \\\n",
    "      songs of {num_total_songs} ({(num_processed_songs / num_total_songs)})\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting MIDI files to NumPy arrays\n",
    "\n",
    "Here we'll take the MIDI files we created with basic pitch and convert them to NumPy arrays that we can use to train our model. The goal is to convert each song into an array with one columns for each time step, one row for each note, and a boolean if a given note was being played at a given timestamp.\n",
    "\n",
    "Basic pitch extracts the velocity, but we're starting with a simple binary representation of the note being played to keep things simple due to hardware limitations. This will result in not needing as much memory (booleans take up less space than integers) and the model will be able to train faster since it does not need to learn what the velocity of a note should be.\n",
    "\n",
    "We'll start with ensuring that the ticks per beat is the same across all of the MIDI files. This is important because TODO: Finish this\n",
    "\n",
    "TODO: Talk about how this is a \"piano roll\"\n",
    "\n",
    "|Pitch   | Timestamp 0 | Timestamp 1 | ... | Timestamp *n* |\n",
    "|--------|-------------|-------------|-----|---------------|\n",
    "| 1      | FALSE       | FALSE       | ... | FALSE         |\n",
    "| 2      | TRUE        | TRUE        | ... | FALSE         |\n",
    "| 3      | FALSE       | FALSE       | ... | TRUE          |\n",
    "| ...    | ...         | ...         | ... | ...           |\n",
    "| 127    | FALSE       | FALSE       | ... | FALSE         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "\n",
    "# Double checking that the ticks per beat are consistent in all of the MIDI files\n",
    "# This will be used to specify the length in seconds to slice by\n",
    "all_ticks_per_beat = []\n",
    "for midi in midis:\n",
    "    mid = mido.MidiFile(midi)\n",
    "    all_ticks_per_beat.append(mid.ticks_per_beat)\n",
    "\n",
    "assert len(set(all_ticks_per_beat)) == 1, \"Different ticks per beat in MIDI files\"\n",
    "ticks_per_beat = all_ticks_per_beat[0]\n",
    "\n",
    "def parse_midi_to_array(file_path: str) -> np.ndarray:\n",
    "    \"\"\"TODO: Write docstring\"\"\"\n",
    "    # Load MIDI file\n",
    "    mid = mido.MidiFile(file_path)\n",
    "\n",
    "    # Initialize the song array with zeros\n",
    "    # The X axis is the timestamp, and the Y axis is the pitch\n",
    "    ticks_per_beat = mid.ticks_per_beat\n",
    "    max_time = int(mid.length * ticks_per_beat)\n",
    "    max_pitch = 127\n",
    "    song_array = np.full((max_time, max_pitch), False)\n",
    "\n",
    "    # Create array of note on/off events\n",
    "    note_events = []\n",
    "    for msg in mid:\n",
    "        if msg.type == \"note_on\":\n",
    "            note_events.append((msg.note, msg.time))\n",
    "        elif msg.type == \"note_off\":\n",
    "            note_events.append((msg.note, msg.time))\n",
    "\n",
    "    # Create array of note times\n",
    "    note_times = []\n",
    "    time = 0\n",
    "    for event in note_events:\n",
    "        note_times.append((event[0], time, event[1]))\n",
    "        time += event[1]\n",
    "\n",
    "    # Create array of note durations\n",
    "    note_durations = []\n",
    "    for i in range(len(note_times)):\n",
    "        note = note_times[i]\n",
    "        if i == len(note_times) - 1:\n",
    "            duration = mid.length - note[1]\n",
    "        else:\n",
    "            duration = note_times[i + 1][1] - note[1]\n",
    "        note_durations.append(duration)\n",
    "\n",
    "    # Populate the song array\n",
    "    for i in range(len(note_times)):\n",
    "        note = note_times[i]\n",
    "        duration = note_durations[i]\n",
    "        start_time = int(note[1] * ticks_per_beat)\n",
    "        end_time = int((note[1] + duration) * ticks_per_beat)\n",
    "        pitch = note[0]\n",
    "        song_array[start_time:end_time, pitch] = True\n",
    "\n",
    "    return song_array\n",
    "\n",
    "\n",
    "# Converting the MIDI files to numpy arrays\n",
    "midi_arrays = []\n",
    "for midi in midis:\n",
    "    midi_arrays.append(parse_midi_to_array(midi))\n",
    "\n",
    "assert len(midi_arrays) == len(midis), \"MIDI arrays and # of MIDI files don't match\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining how long of sequences to use for training and the seeds for the song generation\n",
    "song_snippet_length_s = 5  # In seconds\n",
    "sequence_length = song_snippet_length_s * ticks_per_beat\n",
    "\n",
    "# Taking the list of arrays and converting it to semi-redundant sequences based on the max length of the sequence\n",
    "step_ratio_from_example = 3 / 100  # TODO: Figure out what we should be using instead\n",
    "step = int(step_ratio_from_example * sequence_length)\n",
    "\n",
    "# Getting the number of X axes to be able to create the array to fill\n",
    "# Doing it this way to avoid having to create a list of all the sequences and then convert it to an array which is very memory heavy\n",
    "num_sequences = 0\n",
    "for song in midi_arrays:\n",
    "    for i in range(0, len(song) - sequence_length, step):\n",
    "        num_sequences += 1\n",
    "\n",
    "# Initializing the arrays to fill\n",
    "assert (\n",
    "    len(set([song.shape[1] for song in midi_arrays])) == 1\n",
    "), \"All songs must have the same number of pitches\"\n",
    "num_pitches = midi_arrays[0].shape[1]\n",
    "sequences = np.zeros((num_sequences, sequence_length, num_pitches), dtype=bool)\n",
    "next_notes = np.zeros((num_sequences, num_pitches), dtype=bool)\n",
    "\n",
    "# Iterating through the songs and filling the arrays\n",
    "seq_num = 0\n",
    "for song in midi_arrays:\n",
    "    for i in range(0, len(song) - sequence_length, step):\n",
    "        sequences[seq_num] = song[i : i + sequence_length]\n",
    "        next_notes[seq_num] = song[i + sequence_length]\n",
    "        seq_num += 1\n",
    "\n",
    "# Saving the arrays\n",
    "output_file = os.path.join(cwd, \"Data/training_data.npy\")\n",
    "np.save(output_file, sequences)\n",
    "np.save(output_file.replace(\"_data.npy\", \"_labels.npy\"), next_notes)\n",
    "\n",
    "print(\"Number of sequences:\", len(sequences))\n",
    "print(\"Shape of sequences:\", sequences[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the song generation model\n",
    "\n",
    "Ultimately, we just want a classification model that predicts the probability of each note/pitch being played at each time step. We'll expand upon this more in the song generation section for how to continue generating songs beyond just the next timestamp.\n",
    "\n",
    "There are a variety of models that can be used to generate music. Here we are going to try a few models in increasing levels of complexity. I'm doing this because it helps explain different concepts and components, and it gives options in case you are looking to borrow from this project.\n",
    "\n",
    "Lastly, we're going to use Keras to build the models. Keras is a high-level API (built on top of TensorFlow) that makes it easy to build and train neural networks, and I think the simplicity and readability of the code is preferable for a blog post over more complex libraries like PyTorch or TensorFlow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM\n",
    "\n",
    "This is the most basic model we'll build, and it had surprisingly good results! It's just a single layer [long short-term memory (LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory) network with 56 units that is fed to a dense layer that gives the probability of each note being played at the next timestamp.\n",
    "\n",
    "LSTMs are a type of [recurrent neural network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network) that are designed to handle sequential data by \"remembering\" data from earlier in the sequence. I recommend reading Christopher Olah's [blog post on LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) if you want to learn more. I'm a big fan of his work, and I highly recommend the [Distill](https://distill.pub/) journal he cofounded to help explain research and concepts in a more interactive way than traditional papers. [Here is an example on memory within RNNs](https://distill.pub/2019/memorization-in-rnns/) that is germane to this project.\n",
    "\n",
    "LSTMs are very useful for music generation because what is played at the next timestamp is most likely dependent on more than what was played at the previous timestamp. For example, if a song is in the key of C major, then there are only 7 possible notes that can be played if we ignore accidentals and octaves. If the model only pays attention to the previous note played, then it may not know if the song is in C major and will have a more difficult time distinguishing which notes could be played next. Another example is that songs  often have structure in the form of [chord progressions](https://en.wikipedia.org/wiki/Chord_progression), so if a model is trying to learn a song that is using the 12 bar blues chord progression (I-I-I-I-IV-IV-I-I-V-IV-I-V), then it will likely struggle if it only pays attention to the previous chord played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape of the data for the model\n",
    "sequence_length = sequences.shape[1]\n",
    "num_pitches = sequences.shape[2]\n",
    "\n",
    "# Simple LSTM\n",
    "simple_model = tf.keras.Sequential()\n",
    "simple_model.add(tf.keras.Input(shape=(sequence_length, num_pitches)))\n",
    "simple_model.add(tf.keras.layers.LSTM(56))\n",
    "simple_model.add(tf.keras.layers.Dense(num_pitches, activation=\"softmax\"))\n",
    "simple_model.summary()\n",
    "\n",
    "simple_model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Always use early stopping :)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Training the model w/ early stopping\n",
    "epochs = 10  # Increase if the hardware can handle it\n",
    "batch_size = 128\n",
    "history = simple_model.fit(\n",
    "    sequences,\n",
    "    next_notes,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "# Saving the model\n",
    "model_path = os.path.join(cwd, \"Data/Outputs/simple_LSTM.h5\")\n",
    "simple_model.save(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational autoencoder (VAE)\n",
    "\n",
    "LSTMs can generate decent music, but research has shown other methods to be more effective. One of these methods is a [variational autoencoder (VAE)](https://en.wikipedia.org/wiki/Autoencoder#Variational_autoencoder_(VAE)). This is a type of [autoencoder](https://en.wikipedia.org/wiki/Autoencoder) that is designed to learn a latent space that can be used to generate new data.\n",
    "\n",
    "TODO: Explain what a VAE is\n",
    "\n",
    "This will be loosely based on the [MusicVAE](https://magenta.tensorflow.org/music-vae) model architecture created by Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck. I recommend reading their [paper](https://arxiv.org/abs/1803.05428) and checking out the [blog post on MusicVAE](https://magenta.tensorflow.org/music-vae). We won't directly copy the hierarchical part of their model architecture because I'm not concerned about generating longer samples, but we may play with the latent space from the encoder.\n",
    "\n",
    "Our model will also be much smaller than MusicVAE's due to hardware limitations. MusicVAE's encoder (section 3.1 in their paper) is a two layer LSTM with 2048 units each fed into a fully connected layer with 512 units, and their decoder (section 3.2) is a bit more complex due to the hierarchical nature, but it involves multiple LSTMs with 1024 units each. My computer (that doesn't have a GPU) struggled to train a simple LSTM with 56 units, so we'll stick to a smaller model since I'll be training it on a virtual machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating songs\n",
    "\n",
    "The song generation will be pretty straightforward. We will have to feed the model a \"seed\" of a sequence of notes to predict the next timestamp. We'll then iterate over this to continuously generate the next timestamp given the previous predictions. For example, if our model generates the 6th timestamp given the previous 5 timestamps, then it will generate the 7th timestamp given the timestamps 2-6, and so on. In this example, our model will no longer be using the seed after the fifth prediction. We will repeat this process enough times until we have an adequate length song.\n",
    "\n",
    "Because our model outputs the probability of a note being played, we can randomly sample the predictions to get the notes played at that timestsamp. This would be more difficult if we wanted to capture the velocity of a note, but we're just using a binary representation of whether a note is played or not.\n",
    "\n",
    "We'll begin with gathering our seeds. This will just be the beginning of different songs that were used in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Code here to get the seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_song(seed: np.ndarray, model, ticks_per_beat: int, num_seconds=15) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a song based on a seed array and a trained model\n",
    "\n",
    "    Args:\n",
    "        seed (np.ndarray): The seed array to use to generate the song\n",
    "        model (keras.Model): The trained model to use to generate the song\n",
    "        ticks_per_beat (int): The number of ticks per beat in the MIDI files\n",
    "        num_seconds (int, optional): The number of seconds to generate. Defaults to 15.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The generated song\n",
    "    \"\"\"\n",
    "    song = seed\n",
    "    seq_length = seed.shape[0]\n",
    "    num_notes = num_seconds * ticks_per_beat\n",
    "    for i in range(num_notes):\n",
    "        probabilities = model.predict(song[-seq_length:].reshape(1, seq_length, 127))\n",
    "        notes_played = np.random.binomial(n=1, p=probabilities)\n",
    "        song = np.append(song, notes_played, axis=0)\n",
    "    return song\n",
    "\n",
    "\n",
    "generated_song = generate_song(seed=seed, model=model, ticks_per_beat=ticks_per_beat, num_seconds=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the generated songs as MIDI\n",
    "\n",
    "The generated song is currently just a numpy array, so we'll need to do some work to convert it back to a MIDI file. We'll start by converting from the piano roll array into an array with the start/end time of each note played.\n",
    "\n",
    "So our current generated song is an array that looks like this:\n",
    "\n",
    "|Pitch   | Timestamp 0 | Timestamp 1 | ... | Timestamp *n* |\n",
    "|--------|-------------|-------------|-----|---------------|\n",
    "| 1      | FALSE       | FALSE       | ... | FALSE         |\n",
    "| 2      | TRUE        | TRUE        | ... | FALSE         |\n",
    "| 3      | FALSE       | FALSE       | ... | TRUE          |\n",
    "| ...    | ...         | ...         | ... | ...           |\n",
    "| 127    | FALSE       | FALSE       | ... | FALSE         |\n",
    "\n",
    "And we will convert it to an array that looks like this for all notes that were played:\n",
    "\n",
    "|Pitch     | Start Time | End Time |\n",
    "|----------|------------|----------|\n",
    "| 60       | 0          | 0.5      |\n",
    "| 62       | 2.5        | 4.5      |\n",
    "| 122      | 2.5        | 4.5      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_array_for_note_start_end(\n",
    "    arr: np.ndarray, ticks_per_beat: int = 220\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Find the start and end of consecutive True values in a 2D array.\n",
    "    Thanks ChatGPT!\n",
    "\n",
    "    Args:\n",
    "        arr (np.ndarray): The array to reshape\n",
    "        ticks_per_beat (int, optional): The number of ticks per beat in the MIDI files. Defaults to 220.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reshaped array\n",
    "    \"\"\"\n",
    "    if arr.shape[1] == 127:\n",
    "        arr = arr.T\n",
    "\n",
    "    sequences = []\n",
    "    for row_idx in range(arr.shape[0]):\n",
    "        row = arr[row_idx]\n",
    "        start, end = None, None\n",
    "        for col_idx in range(arr.shape[1]):\n",
    "            if row[col_idx]:\n",
    "                if start is None:\n",
    "                    start = col_idx\n",
    "                end = col_idx\n",
    "            elif start is not None:\n",
    "                sequences.append((row_idx, start, end))\n",
    "                start, end = None, None\n",
    "        if start is not None:\n",
    "            sequences.append((row_idx, start, end))\n",
    "    output_array = np.array(sequences, dtype=float)\n",
    "\n",
    "    # Converting to seconds\n",
    "    output_array[:, 1] /= ticks_per_beat\n",
    "    output_array[:, 2] /= ticks_per_beat\n",
    "\n",
    "    # Ordering the array by start time\n",
    "    output_array = output_array[output_array[:, 1].argsort()]\n",
    "    return output_array  # (pitch, start, end)\n",
    "\n",
    "\n",
    "song_snippet_start_ends = reshape_array_for_note_start_end(song_snippet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do a little more data processing before being able to create a MIDI from our generated song. This is because the time argument for adding notes to a MIDI file with Mido is the time since the last note was played, not the absolute time. So if we have a song that has a note played at 0.5 seconds and another note played at 1.5 seconds, then the time argument for the second note will be 1 second.\n",
    "\n",
    "We'll have to calculate the difference between the start time of each note and the end time of the previous note to get the time argument for each note. Fortunately, this is really easy to do with pandas. We'll start by converting our array of notes into a pandas DataFrame. Next, we'll use the `melt` method to convert the DataFrame from wide to long format where the start and end of each note has its own row. We'll then sort the DataFrame by the timestamp and lag the time of the next note with the `shift` method. Finally, we'll calculate the difference between the start time of each note and the end time of the previous note.\n",
    "\n",
    "Our array of notes will now become a data frame that looks like this:\n",
    "\n",
    "|Pitch     | Type | Time | lagged_time | time_delta|\n",
    "|----------|------|------|-------------|-----------|\n",
    "| 60       | start| 0    | np.NaN      | 0         |\n",
    "| 60       | end  | 0.5  | 0           | 0.5       |\n",
    "| 62       | start| 2.5  | 0.5         | 2.0       |\n",
    "| 122      | start| 2.5  | 2.5         | 0         |\n",
    "| 62       | end  | 4.5  | 2.5         | 2.0       |\n",
    "| 122      | end  | 4.5  | 4.5         | 0         |\n",
    "\n",
    "And we can now use the `time_delta` column to add the notes to the MIDI file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_note_time_deltas(note_start_end_array: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Further formats the array to get time delta between each note to prep the data for MIDI conversion\n",
    "\n",
    "    Args:\n",
    "        note_start_end_array (np.ndarray): The array to reshape from reshape_array_for_note_start_end()\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reshaped array\n",
    "    \"\"\"\n",
    "    # Formatting further to convert to MIDI\n",
    "    # This is because we need the time delta between each note and if that note was a start or end\n",
    "    # Using Pandas to do the remainder of the processing due to ease of use\n",
    "    midi_df = pd.DataFrame(note_start_end_array, columns=[\"pitch\", \"start\", \"end\"])\n",
    "    midi_df = midi_df.melt(id_vars=\"pitch\").rename(\n",
    "        columns={\"variable\": \"type\", \"value\": \"time\"}\n",
    "    )\n",
    "    midi_df = midi_df.sort_values(by=\"time\").reset_index(drop=True)\n",
    "    midi_df[\"lagged_time\"] = midi_df[\"time\"].shift(-1)\n",
    "    midi_df[\"time_delta\"] = (midi_df[\"lagged_time\"] - midi_df[\"time\"]).fillna(0)\n",
    "    return midi_df\n",
    "\n",
    "\n",
    "song_snippet_for_output = get_note_time_deltas(song_snippet_start_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(\n",
    "    df: pd.DataFrame, midi_path: str, ticks_per_beat: int = 220, tempo: int = 500000\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a MIDI file from a dataframe of notes from get_note_time_deltas()\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe of notes from get_note_time_deltas()\n",
    "        midi_path (str): The path to save the MIDI file to\n",
    "        ticks_per_beat (int, optional): The number of ticks per beat in the MIDI files. Defaults to 220.\n",
    "        tempo (int, optional): The tempo of the MIDI file. Defaults to 500000.\n",
    "\n",
    "    Returns:\n",
    "        None: Saves the MIDI file to the specified path\n",
    "    \"\"\"\n",
    "    mid = mido.MidiFile()\n",
    "    track = mido.MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "    track.append(mido.MetaMessage(\"set_tempo\", tempo=tempo, time=0))\n",
    "    mid.ticks_per_beat = ticks_per_beat\n",
    "    for row in df.itertuples():\n",
    "        if row.type == \"start\":\n",
    "            track.append(\n",
    "                mido.Message(\n",
    "                    \"note_on\",\n",
    "                    note=int(row.pitch),\n",
    "                    velocity=64,\n",
    "                    time=int((row.time_delta * ticks_per_beat * 2)),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            track.append(\n",
    "                mido.Message(\n",
    "                    \"note_on\",\n",
    "                    note=int(row.pitch),\n",
    "                    velocity=0,\n",
    "                    time=int((row.time_delta * ticks_per_beat) * 2),\n",
    "                )\n",
    "            )\n",
    "    mid.save(midi_path)\n",
    "    print(f\"MIDI file saved to {midi_path}\")\n",
    "    \n",
    "\n",
    "midi_output_path = generated_song_path.replace(\".npy\", \".mid\")\n",
    "create_midi(song_snippet_for_output, midi_output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to audio\n",
    "\n",
    "How has nobody named software dealing with audio files \"audiofile\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
